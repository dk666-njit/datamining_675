# -*- coding: utf-8 -*-
"""dinesh_murthy_kadali_midtermproj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tety-3_Z9xAiIKSQddi7qasy1CE7_vww
"""

# Import necessary libraries
import pandas as pd
from itertools import combinations
from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth
from mlxtend.preprocessing import TransactionEncoder
import time

# Step 1: Data Loading and Preprocessing

class TransactionDataLoader:
    def __init__(self, data_sources):
        self.data_sources = data_sources

    def list_available_sources(self):
        print("\nAvailable Datasets:")
        for idx, source in enumerate(self.data_sources.keys(), 1):
            print(f"{idx}. {source}")
        print("0. Exit")

    def load_dataset(self, choice):
        selected_dataset = list(self.data_sources.keys())[choice - 1]
        print(f"Loading dataset: {selected_dataset}")
        return self._read_csv(self.data_sources[selected_dataset]), selected_dataset

    def _read_csv(self, file_path):
        df = pd.read_csv(file_path)
        return df['Items'].apply(lambda items: items.split(', ')).tolist()


# Step 2: Frequent Itemset Mining Algorithms

class FrequentItemsetAlgorithms:

    @staticmethod
    def brute_force(transactions, min_support_threshold):
        return FrequentItemsetAlgorithms._generate_frequent_itemsets(transactions, min_support_threshold)

    @staticmethod
    def apriori(transactions, support_threshold, confidence_threshold):
        df = FrequentItemsetAlgorithms._encode_transactions(transactions)
        frequent_itemsets = apriori(df, min_support=support_threshold, use_colnames=True)
        rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=confidence_threshold)
        return frequent_itemsets, rules

    @staticmethod
    def fp_growth(transactions, support_threshold, confidence_threshold):
        df = FrequentItemsetAlgorithms._encode_transactions(transactions)
        frequent_itemsets = fpgrowth(df, min_support=support_threshold, use_colnames=True)
        rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=confidence_threshold)
        return frequent_itemsets, rules

    @staticmethod
    def _generate_frequent_itemsets(transactions, min_support_threshold):
        item_counter = {}
        for transaction in transactions:
            for item in transaction:
                item_counter[item] = item_counter.get(item, 0) + 1

        frequent_itemsets = {1: {item: count for item, count in item_counter.items() if count / len(transactions) >= min_support_threshold}}
        size = 2

        while True:
            prev_itemsets = list(frequent_itemsets[size - 1].keys())
            candidate_combinations = list(combinations(prev_itemsets, size))
            item_counter = {}
            for transaction in transactions:
                transaction_set = set(transaction)
                for itemset in candidate_combinations:
                    if set(itemset).issubset(transaction_set):
                        item_counter[itemset] = item_counter.get(itemset, 0) + 1

            frequent_itemsets[size] = {itemset: count for itemset, count in item_counter.items() if count / len(transactions) >= min_support_threshold}
            if not frequent_itemsets[size]:
                del frequent_itemsets[size]
                break
            size += 1
        return frequent_itemsets

    @staticmethod
    def _encode_transactions(transactions):
        encoder = TransactionEncoder()
        transformed_data = encoder.fit(transactions).transform(transactions)
        return pd.DataFrame(transformed_data, columns=encoder.columns_)


# Step 3: Timing Wrapper

class AlgorithmTimer:

    @staticmethod
    def time_algorithm(algorithm_function, *args):
        start_time = time.time()
        result = algorithm_function(*args)
        elapsed_time = time.time() - start_time
        return result, elapsed_time


# Step 4: User Interaction and Execution Flow

class FrequentItemsetAnalyzer:

    def __init__(self, data_loader, algorithm_module, timer):
        self.data_loader = data_loader
        self.algorithm_module = algorithm_module
        self.timer = timer

    def run(self):
        while True:
            self.data_loader.list_available_sources()
            choice = int(input("Select a dataset by entering its number (or 0 to exit): "))
            if choice == 0:
                print("Exiting the program.")
                break

            transactions, dataset_name = self.data_loader.load_dataset(choice)
            print(f"Loaded {len(transactions)} transactions from {dataset_name}.")

            support_threshold = float(input("Enter minimum support threshold (as a percentage, e.g., 10 for 10%): ")) / 100
            confidence_threshold = float(input("Enter minimum confidence threshold (as a percentage, e.g., 20 for 20%): ")) / 100

            print(f"\nAnalyzing {dataset_name} with support={support_threshold * 100}%, confidence={confidence_threshold * 100}%...\n")

            self._run_algorithms(transactions, support_threshold, confidence_threshold)

            continue_choice = input("Do you want to analyze another dataset? (yes/no): ").strip().lower()
            if continue_choice != 'yes':
                print("Program terminated.")
                break

    def _run_algorithms(self, transactions, support_threshold, confidence_threshold):
        # Brute Force
        bf_results, bf_time = self.timer.time_algorithm(self.algorithm_module.brute_force, transactions, support_threshold)
        print(f"Brute Force Frequent Itemsets:\n{bf_results}")
        print(f"Brute Force Execution Time: {bf_time:.4f} seconds\n")

        # Apriori
        apriori_results, apriori_time = self.timer.time_algorithm(self.algorithm_module.apriori, transactions, support_threshold, confidence_threshold)
        print(f"Apriori Frequent Itemsets:\n{apriori_results[0]}")
        print(f"Apriori Association Rules:\n{apriori_results[1]}")
        print(f"Apriori Execution Time: {apriori_time:.4f} seconds\n")

        # FP-Growth
        fp_results, fp_time = self.timer.time_algorithm(self.algorithm_module.fp_growth, transactions, support_threshold, confidence_threshold)
        print(f"FP-Growth Frequent Itemsets:\n{fp_results[0]}")
        print(f"FP-Growth Association Rules:\n{fp_results[1]}")
        print(f"FP-Growth Execution Time: {fp_time:.4f} seconds\n")


# Initialization and Execution

if __name__ == "__main__":
    # Define datasets
    datasets = {
        "Amazon": r"/content/amazon.csv",
        "BestBuy": r"/content/bestbuy.csv",
        "KMart": r"/content/kmart.csv",
        "Nike": r"/content/nike.csv"
    }

    # Instantiate loader, algorithms, timer, and analyzer
    data_loader = TransactionDataLoader(datasets)
    algorithm_module = FrequentItemsetAlgorithms()
    timer = AlgorithmTimer()

    # Create and run the analyzer
    analyzer = FrequentItemsetAnalyzer(data_loader, algorithm_module, timer)
    analyzer.run()