# -*- coding: utf-8 -*-
"""Dinesh_DM_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xLdaFKyZ9J5HsMnBjIIm9pF8R-jgAuKz
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
import seaborn as sns
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, brier_score_loss, auc
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM

!pip install gdown

def load_and_preprocess_data(url):
    !gdown https://drive.google.com/uc?id={url} -O "url.csv"
    data = pd.read_csv("url.csv")

    # Print the column names to check for any discrepancies
    print("Columns in the dataset:", data.columns)

    # Select relevant columns for the new dataset
    selected_columns = ['GENDER', 'AGE', 'SMOKING', 'YELLOW_FINGERS', 'ANXIETY',
                        'PEER_PRESSURE', 'CHRONIC DISEASE', 'FATIGUE ', 'ALLERGY ',
                        'WHEEZING', 'ALCOHOL CONSUMING', 'COUGHING', 'SHORTNESS OF BREATH',
                        'SWALLOWING DIFFICULTY', 'CHEST PAIN', 'LUNG_CANCER']

    # Ensure that all selected columns exist in the dataset
    missing_columns = [col for col in selected_columns if col not in data.columns]
    if missing_columns:
        print(f"Warning: The following columns are missing: {missing_columns}")
        # Optionally, remove missing columns from the selected_columns list
        selected_columns = [col for col in selected_columns if col in data.columns]

    data = data[selected_columns]

    # Map target 'LUNG_CANCER' to binary (Yes = 1, No = 0)
    data['LUNG_CANCER'] = data['LUNG_CANCER'].map({'YES': 1, 'NO': 0})

    # Encode categorical columns (e.g., 'GENDER', 'SMOKING', etc.)
    label_encoder = LabelEncoder()
    categorical_columns = ['GENDER', 'SMOKING', 'YELLOW_FINGERS', 'ANXIETY', 'PEER_PRESSURE',
                           'CHRONIC DISEASE', 'FATIGUE ', 'ALLERGY ', 'WHEEZING', 'ALCOHOL CONSUMING',
                           'COUGHING', 'SHORTNESS OF BREATH', 'SWALLOWING DIFFICULTY', 'CHEST PAIN']

    for column in categorical_columns:
        data[column] = label_encoder.fit_transform(data[column])

    return data

def calculate_metrics(conf_matrix, y_true=None, y_prob=None):
    """Calculate evaluation metrics."""
    TP, TN, FP, FN = conf_matrix[1, 1], conf_matrix[0, 0], conf_matrix[0, 1], conf_matrix[1, 0]
    sensitivity = TP / (TP + FN)
    specificity = TN / (TN + FP)
    precision = TP / (TP + FP) if (TP + FP) else 0
    recall = sensitivity
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    balanced_accuracy = (sensitivity + specificity) / 2
    tss = sensitivity + specificity - 1
    hss_num = 2 * (TP * TN - FP * FN)
    hss_denom = (TP + FN) * (FP + TN) + (TP + FP) * (FN + TN)
    hss = hss_num / hss_denom if hss_denom else 0
    bs = np.mean((y_true - y_prob) ** 2) if y_true is not None and y_prob is not None else None
    bss_denom = np.mean((y_true - np.mean(y_true)) ** 2) if y_true is not None else None
    bss = 1 - (bs / bss_denom) if bs is not None and bss_denom else None
    return {
        'Sensitivity': sensitivity, 'Specificity': specificity, 'Accuracy': accuracy,
        'Precision': precision, 'Recall': recall, 'F1 Score': f1,
        'Balanced Accuracy': balanced_accuracy, 'TSS': tss, 'HSS': hss,
        'Brier Score': bs, 'Brier Skill Score': bss
    }

def train_model(model, X_train, y_train, X_test, y_test):
    """Train and evaluate a model."""
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    conf_matrix = confusion_matrix(y_test, predictions)
    metrics = calculate_metrics(conf_matrix)
    auc = roc_auc_score(y_test, predictions)
    brier = brier_score_loss(y_test, predictions)
    return metrics, auc, brier

def build_lstm_model(input_shape):
    """Build an LSTM model."""
    model = Sequential([
        LSTM(50, input_shape=input_shape),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Load and preprocess the dataset
dataset_url = '1EbM9lKDB8f_PegVbKxsNmBaTzpRikksz'
data = load_and_preprocess_data(dataset_url)

# Separate features and target
X, y = data.drop(columns=['LUNG_CANCER']), data['LUNG_CANCER']

# Apply StandardScaler only to the numerical columns
numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns
X[numerical_columns] = StandardScaler().fit_transform(X[numerical_columns])

X_scaled = X.dropna()
y = y.dropna()
y = y.reset_index(drop=True)  # Reset index of y
X_scaled = X_scaled.reset_index(drop=True)  # Reset index of X_scaled
y = y.squeeze()  # Converts DataFrame to Series if y is a DataFrame
X_scaled = X_scaled.to_numpy()  # Convert X_scaled to numpy array for proper slicing

# Handling any mismatch between X and y
if len(X_scaled) != len(y):
    # Randomly sample data to make the lengths equal
    diff = abs(len(X_scaled) - len(y))
    random_indices = np.random.choice(len(X_scaled), diff)
    X_scaled = np.concatenate([X_scaled, X_scaled[random_indices]], axis=0)
    y = np.concatenate([y, y[random_indices]])

# Check that X_scaled and y have the same number of samples
assert len(y) == len(X_scaled), "Mismatch in number of samples between X and y"

# Initialize KFold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# Initialize lists to store metrics
rf_metrics_list, svm_metrics_list, lstm_metrics_list = [], [], []
rf_brier_score, svm_brier_score, lstm_brier_score = [], [], []

# Cross-validation
for fold, (train_index, test_index) in enumerate(kf.split(X_scaled), 1):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y[train_index], y[test_index]

    print(f"\nFold {fold}:")

    # Ensure target_classes are not empty
    target_classes = y_train.dropna().unique()

    # Fill NaN values in y_train and y_test with random values from the target classes
    if target_classes.size == 0:
        target_classes = [0]  # Replace with a default value
    y_train = y_train.apply(lambda x: np.random.choice(target_classes) if pd.isnull(x) else x)
    y_test = y_test.apply(lambda x: np.random.choice(target_classes) if pd.isnull(x) else x)

    # Ensure no NaN values in the target columns
    print("Missing values in y_train:", y_train.isnull().sum())
    print("Missing values in y_test:", y_test.isnull().sum())

    # Random Forest
    rf_model = RandomForestClassifier(random_state=1)
    rf_model.fit(X_train, y_train)
    rf_pred = rf_model.predict(X_test)
    rf_conf_matrix = confusion_matrix(y_test, rf_pred)
    rf_metrics = calculate_metrics(rf_conf_matrix)
    rf_metrics_list.append(rf_metrics)
    rf_brier_score.append(brier_score_loss(y_test, rf_pred))
    print(f"Random Forest Confusion Matrix:\n{rf_conf_matrix}")
    print(f"Random Forest Metrics: {rf_metrics}")

    # SVM
    svm_model = SVC(probability=True)
    svm_model.fit(X_train, y_train)
    svm_pred = svm_model.predict(X_test)
    svm_conf_matrix = confusion_matrix(y_test, svm_pred)
    svm_metrics = calculate_metrics(svm_conf_matrix)
    svm_metrics_list.append(svm_metrics)
    svm_brier_score.append(brier_score_loss(y_test, svm_pred))
    print(f"SVM Confusion Matrix:\n{svm_conf_matrix}")
    print(f"SVM Metrics: {svm_metrics}")

    # LSTM
    lstm_model = build_lstm_model((X_train.shape[1], 1))
    X_train_lstm = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_lstm = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    lstm_model.fit(X_train_lstm, y_train, epochs=10, batch_size=32, verbose=0)
    lstm_pred = (lstm_model.predict(X_test_lstm) > 0.5).astype(int)
    lstm_conf_matrix = confusion_matrix(y_test, lstm_pred)
    lstm_metrics = calculate_metrics(lstm_conf_matrix)
    lstm_metrics_list.append(lstm_metrics)
    lstm_brier_score.append(brier_score_loss(y_test, lstm_pred))
    print(f"LSTM Confusion Matrix:\n{lstm_conf_matrix}")
    print(f"LSTM Metrics: {lstm_metrics}")

# Convert lists to DataFrame for better presentation
rf_metrics_df = pd.DataFrame(rf_metrics_list)
svm_metrics_df = pd.DataFrame(svm_metrics_list)
lstm_metrics_df = pd.DataFrame(lstm_metrics_list)

# Calculate average metrics for each model
print("\nAverage Metrics for Random Forest:")
print(rf_metrics_df.mean())
print("\nAverage Metrics for SVM:")
print(svm_metrics_df.mean())
print("\nAverage Metrics for LSTM:")
print(lstm_metrics_df.mean())

# Visualize the results
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.boxplot(data=[rf_brier_score, svm_brier_score, lstm_brier_score],
            ax=plt.gca(), palette='Set2')
plt.xticks([0, 1, 2], ['Random Forest', 'SVM', 'LSTM'])
plt.title('Brier Scores')
plt.tight_layout()
plt.show()

# Convert the list of dictionaries into a DataFrame for each model
rf_metrics_df = pd.DataFrame(rf_metrics_list)
svm_metrics_df = pd.DataFrame(svm_metrics_list)
lstm_metrics_df = pd.DataFrame(lstm_metrics_list)

# Calculate average metrics for each model
rf_avg_metrics = rf_metrics_df.mean()
svm_avg_metrics = svm_metrics_df.mean()
lstm_avg_metrics = lstm_metrics_df.mean()

# Create a comparison DataFrame
comparison_df = pd.DataFrame({
    'Random Forest': rf_avg_metrics,
    'SVM': svm_avg_metrics,
    'LSTM': lstm_avg_metrics
})

# Display the comparison table
print("Comparison of average metrics:")
print(comparison_df)

# ROC and AUC
plt.figure(figsize=(10, 6))

# Compute ROC curves for each model (using predicted probabilities for each model)
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_pred)
fpr_svm, tpr_svm, _ = roc_curve(y_test, svm_pred)
fpr_lstm, tpr_lstm, _ = roc_curve(y_test, lstm_pred)

# Calculate AUC
rf_roc_auc = auc(fpr_rf, tpr_rf)
svm_roc_auc = auc(fpr_svm, tpr_svm)
lstm_roc_auc = auc(fpr_lstm, tpr_lstm)

# Print TPR and FPR for each model
print("\nTrue Positive Rate (TPR) and False Positive Rate (FPR):")
print(f"Random Forest - TPR: {tpr_rf}")
print(f"Random Forest - FPR: {fpr_rf}")
print(f"SVM - TPR: {tpr_svm}")
print(f"SVM - FPR: {fpr_svm}")
print(f"LSTM - TPR: {tpr_lstm}")
print(f"LSTM - FPR: {fpr_lstm}")

# Plot ROC curves
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {np.mean(rf_roc_auc):.2f})', color='blue')
plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC = {np.mean(svm_roc_auc):.2f})', color='green')
plt.plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {np.mean(lstm_roc_auc):.2f})', color='red')
plt.plot([0, 1], [0, 1], linestyle='--', label='Random (AUC = 0.5)', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend(loc='lower right')
plt.show()

# Display Brier Scores
print(f"\nAverage Brier Score - Random Forest: {np.mean(rf_brier_score):.4f}")
print(f"Average Brier Score - SVM: {np.mean(svm_brier_score):.4f}")
print(f"Average Brier Score - LSTM: {np.mean(lstm_brier_score):.4f}")